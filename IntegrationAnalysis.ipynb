{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML \n",
    "display(HTML(\n",
    "    \"<style>.container {  !important;\\\n",
    "    } div.output_wrapper .output { padding-left: 14px; }</style>\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import auc, roc_auc_score, adjusted_mutual_info_score\n",
    "from scipy.stats import ks_2samp, chi2_contingency, ttest_rel, ttest_1samp, ttest_ind\n",
    "from cds.utilities import read_hdf5, write_hdf5\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility and Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hdf5(df, filename):\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    dest = h5py.File(filename)\n",
    "\n",
    "    try:\n",
    "        dim_0 = [x.encode('utf8') for x in df.index]\n",
    "        dim_1 = [x.encode('utf8') for x in df.columns]\n",
    "\n",
    "        dest_dim_0 = dest.create_dataset('dim_0', track_times=False, data=dim_0)\n",
    "        dest_dim_1 = dest.create_dataset('dim_1', track_times=False, data=dim_1)\n",
    "        dest.create_dataset(\"data\", track_times=False, data=df.values)\n",
    "    finally:\n",
    "        dest.close()\n",
    "\n",
    "def read_hdf5(filename):\n",
    "    src = h5py.File(filename, 'r')\n",
    "    try:\n",
    "        dim_0 = [x.decode('utf8') for x in src['dim_0']]\n",
    "        dim_1 = [x.decode('utf8') for x in src['dim_1']]\n",
    "        data = np.array(src['data'])\n",
    "\n",
    "        return pd.DataFrame(index=dim_0, columns=dim_1, data=data)\n",
    "    finally:\n",
    "        src.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_melt(df, **kwargs):\n",
    "    name = df.index.name\n",
    "    if not name:\n",
    "        name = 'index'\n",
    "    return pd.melt(df.reset_index(level=(df.index.nlevels-1)), id_vars=name, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(positive_controls, negative_controls):\n",
    "    return roc_auc_score(\n",
    "        [0]*len(positive_controls) + [1] * len(negative_controls), \n",
    "        list(positive_controls) + list(negative_controls)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_pval(observed, null):\n",
    "    '''\n",
    "    generates left-tailed pvalues\n",
    "    '''\n",
    "    null = null.dropna().sort_values()\n",
    "    observed = observed.dropna()\n",
    "    ind = observed.index.copy()\n",
    "    observed.sort_values(inplace=True)\n",
    "    return pd.Series(\n",
    "        (np.searchsorted(null, observed)+1)/(len(null)+1),\n",
    "        index=observed.index\n",
    "    ).reindex(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_fdr(observed, null):\n",
    "    pval =  empirical_pval(observed, null)\n",
    "    return pd.Series(\n",
    "        fdrcorrection( pval)[1], \n",
    "        index=pval.index\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnmd(x):\n",
    "    return (x[ess].mean() - x[ness].mean())/x[ness].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind(pairs, keep):\n",
    "    '''\n",
    "    pairs: list of tuple of strings indicating column pairs to check\n",
    "    keep: list of strings indicating columns to keep\n",
    "    '''\n",
    "    keep_mapping = pd.Series(\n",
    "        np.arange(len(keep)),\n",
    "        index=keep\n",
    "    ).sort_index()\n",
    "    return (keep_mapping.loc[[v[0] for v in pairs]].values*len(keep_mapping) +\n",
    "            keep_mapping.loc[[v[1] for v in pairs]].values).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_cor_no_missing(x, y):\n",
    "    \"\"\"Full column-wise Pearson correlations of two matrices with no missing values.\"\"\"\n",
    "    xv = (x - x.mean(axis=0))/x.std(axis=0)\n",
    "    yv = (y - y.mean(axis=0))/x.std(axis=0)\n",
    "    result = np.dot(xv.T, yv)/len(xv)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_dist(df, pairs):\n",
    "    '''\n",
    "    df: matrix\n",
    "    pairs: dict of pair types\n",
    "    '''\n",
    "    out = {}\n",
    "    print('calculating correlations')\n",
    "    covs = np.ravel(np_cor_no_missing(\n",
    "        df.values, \n",
    "        df.values\n",
    "    ))\n",
    "    print('subsetting')\n",
    "    for key, pairset in pairs.items():\n",
    "        ind = get_ind(pairset, df.columns)\n",
    "        out[key] = pd.Series(covs[ind], index=pairset)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose preprocessings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to a local directory where you've stored the data files from Figshare\n",
    "source = './Data_23.9.20/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    metadata = pd.read_csv(source + 'MetaSamples.csv')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You will need to download the file MetaSamples.csv from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\")\n",
    "    \n",
    "files = {}\n",
    "for ind, row in metadata.iterrows():\n",
    "    if not row.postcorrection in files:\n",
    "        files[row.postcorrection] = {}\n",
    "    files[row.postcorrection][row.preprocessing] = os.path.join(source, row.Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'AnalysisOutput'\n",
    "if not os.path.isdir(output):\n",
    "    os.mkdir(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessings = ['CRISPRCleanR', 'CCR-JACKS', 'CERES']#metadata['preprocessing'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcorrections = ['ComBat', 'ComBat+QN', 'ComBat+QN+PC1', 'ComBat+QN+PC1-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npreprocessings = len(preprocessings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gene Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_effects = {key: {} for key in files.keys()}\n",
    "for postcorrection, f in files.items():\n",
    "    for dataset, v in f.items():\n",
    "        try:\n",
    "            gene_effects[postcorrection][dataset] = read_hdf5(v).T\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"You will need to download the file %s from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\" % v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gene_map = pd.read_csv(source + \"DepMap_gene_map.csv\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You will need to download the file DepMap_gene_map.csv from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_genes = set.intersection(*[\n",
    "    set(v.columns) \n",
    "    for val in gene_effects.values() \n",
    "for v in val.values() ])\n",
    "#shared_genes = set(guide_gene_map.gene) & shared_genes\n",
    "shared_genes = shared_genes & set(gene_map['symbol'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in gene_effects.values():\n",
    "    for v2 in v.values():\n",
    "        v2.drop(sorted(set(v2.columns) - set(gene_map.symbol)), axis=1, inplace=True)\n",
    "        v2.columns = gene_map.set_index('symbol').loc[v2.columns, 'gene'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    si = pd.read_csv(source + \"sample_info.csv\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You will need to download the file sample_info.csv from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = si[['DepMap_ID', 'Sanger_Model_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line_map = pd.concat([\n",
    "    pd.Series(lines['DepMap_ID'].values, index=lines['DepMap_ID'].values),\n",
    "    pd.Series(lines.dropna()['DepMap_ID'].values, index=lines.dropna()['Sanger_Model_ID'].values),\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = set([])\n",
    "for v in gene_effects.values():\n",
    "    for v2 in v.values():\n",
    "        v2.index = cell_line_map.loc[v2.index].values\n",
    "        all_lines.update(set(v2.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npostcorrections = len(gene_effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_genes = set.intersection(*[\n",
    "    set(v.columns) \n",
    "    for val in gene_effects.values() \n",
    "for v in val.values() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ess = pd.read_csv(source + \"depmap_common_essentials.csv\").iloc[:, 0]\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You will need to download the file depmap_common_essentials.csv from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\")\n",
    "ess = [s for s in ess if s in shared_genes]\n",
    "\n",
    "try:\n",
    "    ness = pd.read_csv(source + \"depmap_nonessentials.csv\").iloc[:, 0]\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You will need to download the file depmap_nonessentials.csv from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\")\n",
    "    \n",
    "ness = [s for s in ness if s in shared_genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.set_index('DepMap_ID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in gene_effects.values():\n",
    "    for v2 in v.values():\n",
    "        v2 -= v2[ness].median(axis=1).median()\n",
    "        v2 /= v2[ess].median(axis=1).abs().median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Omics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    related_genes = pd.read_csv(source + \"related_genes.csv\")[['target', 'partner']]\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You will need to download the file related_genes.csv from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\")\n",
    "\n",
    "related_genes = related_genes[related_genes.target.isin(shared_genes) & related_genes.partner.isin(shared_genes)]\n",
    "\n",
    "related_genes.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    expression = read_hdf5(source + \"DepMap_20Q2_expression.hdf5\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You will need to download the file DepMap_20Q2_expression.hdf5 from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mut = pd.read_csv(source + \"DepMap_20Q2_mutations.csv\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You will need to download the file DepMap_20Q2_mutations.csv from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\")\n",
    "\n",
    "mut = mut[mut.DepMap_ID.isin(all_lines)]\n",
    "\n",
    "mut['Gene'] = mut.apply(lambda x: '%s (%i)' % (x['Hugo_Symbol'], x['Entrez_Gene_Id']), axis=1)\n",
    "\n",
    "mut['hotspot'] = mut['isTCGAhotspot'] | mut['isCOSMIChotspot']\n",
    "\n",
    "mut = mut[mut.hotspot].drop_duplicates(subset=['Gene', 'DepMap_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fusions = pd.read_csv(source + \"DepMap_20Q2_fusions.csv\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"You will need to download the file DepMap_20Q2_fusions.csv from the \\\n",
    "figshare at https://figshare.com/projects/Integrated_CRISPR/78252 and save it in the source \\\n",
    "directory\")\n",
    "\n",
    "fusions = fusions.set_index('DepMap_ID').drop(\n",
    "    ['Unnamed: 0'], errors='ignore', axis=1)\n",
    "\n",
    "fusions['LeftGene'] = fusions.LeftGene.apply(lambda s: s.split(' ')[0])\n",
    "fusions['RightGene'] = fusions.RightGene.apply(lambda s: s.split(' ')[0])\n",
    "\n",
    "fusions['Sorted'] = fusions.apply(lambda x: tuple(sorted(x[['LeftGene','RightGene']])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unexpressed False Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ux = (expression < .01).astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ux = {}\n",
    "total_ux = {}\n",
    "threshold = 0.15\n",
    "for postcorrection, coll in gene_effects.items():\n",
    "    \n",
    "    top_ux[postcorrection] = {}\n",
    "    total_ux[postcorrection] = {}\n",
    "    print(postcorrection)\n",
    "    for key in coll.keys():\n",
    "        ux_shared = sorted(set(ux.columns) & set(coll[key].columns))\n",
    "        print('\\t', key)\n",
    "        val = coll[key][ux_shared]\n",
    "        rank = val.rank(pct=True, axis=1)\n",
    "        mask = ux.loc[rank.index, rank.columns]\n",
    "        selected = rank.mask(~mask.fillna(False))\n",
    "        top_ux[postcorrection][key] = (selected < threshold).sum().sum()\n",
    "        total_ux[postcorrection][key] = mask.notnull().sum().sum()\n",
    "        print('\\t', top_ux[postcorrection][key], total_ux[postcorrection][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = index_melt( pd.DataFrame(top_ux)/pd.DataFrame(total_ux))\n",
    "df.columns = ['preprocessing', 'postcorrection', 'UXFP']\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(6, 7))\n",
    "sns.barplot(data=df, x='preprocessing', hue='postcorrection', y='UXFP', \n",
    "            order=preprocessings, hue_order=postcorrections)\n",
    "sns.despine(top=True, right=True)\n",
    "plt.xlabel('')\n",
    "plt.ylabel(\"Unexpressed False Positives\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', which='major')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + \"unexpressed_bar.pdf\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby(['preprocessing', 'postcorrection']).UXFP.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_between_preprocessing = []\n",
    "for postcorrection in postcorrections:\n",
    "    for i, d1 in enumerate(preprocessings[:-1]):\n",
    "        for d2 in preprocessings[i+1:]:\n",
    "            table = [\n",
    "                [top_ux[postcorrection][d1], total_ux[postcorrection][d1] - top_ux[postcorrection][d1]],\n",
    "                [top_ux[postcorrection][d2], total_ux[postcorrection][d2] - top_ux[postcorrection][d2]],\n",
    "            ]\n",
    "            g, p, dof, expected = chi2_contingency(table)\n",
    "            compare_between_preprocessing.append({\"Correction\": postcorrection, \"Preprocessing1\": d1, \n",
    "                                           \"Preprocessing2\": d2, 'p': p})\n",
    "compare_between_preprocessing = pd.DataFrame(compare_between_preprocessing)\n",
    "\n",
    "compare_between_correction = []\n",
    "for dataset in preprocessings:\n",
    "    for i, postcorrection1 in enumerate(postcorrections[:-1]):\n",
    "        for postcorrection2 in postcorrections[i+1:]:\n",
    "            table = [\n",
    "                [top_ux[postcorrection1][dataset], total_ux[postcorrection1][dataset] - top_ux[postcorrection1][dataset]],\n",
    "                [top_ux[postcorrection2][dataset], total_ux[postcorrection2][dataset] - top_ux[postcorrection2][dataset]],\n",
    "            ]\n",
    "            g, p, dof, expected = chi2_contingency(table)\n",
    "            compare_between_correction.append({\"Preprocessing\": dataset, \"Correction1\": postcorrection1, \n",
    "                                           \"Correction2\": postcorrection2, 'p': p})\n",
    "compare_between_correction = pd.DataFrame(compare_between_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_between_preprocessing.groupby([\"Preprocessing1\", \"Preprocessing2\"]).p.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_between_correction.groupby([\"Correction1\", \"Correction2\"]).p.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_at_10fdr = {}\n",
    "for postcorrection in gene_effects.keys():\n",
    "    print(postcorrection)\n",
    "    recall_at_10fdr[postcorrection] = {}\n",
    "    for dataset in gene_effects[postcorrection].keys():\n",
    "        ux_shared = sorted(\n",
    "            set(gene_effects[postcorrection][dataset].columns) \n",
    "            & set(ux.columns)\n",
    "        )\n",
    "        print('\\t', dataset)\n",
    "        val = gene_effects[postcorrection][dataset]\n",
    "        overlap = sorted(set(ux.index) & set(val.index))\n",
    "        recall_at_10fdr[postcorrection][dataset] = val.loc[overlap, ux_shared].apply(lambda x: (\n",
    "            empirical_fdr(\n",
    "                x,\n",
    "                x.iloc[ux.loc[x.name, x.index].fillna(False).values])[ess] < .1\n",
    "            ).mean(),\n",
    "        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall10_flat = pd.concat([\n",
    "    pd.DataFrame({'postcorrection': postcorrection, 'preprocessing': dataset, 'Recall': recall_at_10fdr[postcorrection][dataset]})\n",
    "    for postcorrection, v in recall_at_10fdr.items()\n",
    "    for dataset in v.keys()\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.boxplot(data=recall10_flat, hue='postcorrection', x='preprocessing', y='Recall', \n",
    "            order=preprocessings, hue_order=postcorrections)\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y')\n",
    "plt.ylabel(\"Recall of Essentials at 10% FDR\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + \"recall_10fdr.pdf\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for postcorrection, r in recall_at_10fdr.items():\n",
    "    print(postcorrection)\n",
    "    for dataset in recall_at_10fdr[postcorrection].keys():\n",
    "        print('\\t', dataset, r[dataset].median(), r[dataset].mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_between_preprocessing = []\n",
    "for postcorrection in postcorrections:\n",
    "    for i, d1 in enumerate(preprocessings[:-1]):\n",
    "        for d2 in preprocessings[i+1:]:\n",
    "            t, p = ttest_rel(recall_at_10fdr[postcorrection][d1], recall_at_10fdr[postcorrection][d2])\n",
    "            compare_between_preprocessing.append({\"Correction\": postcorrection, \"Preprocessing1\": d1,\n",
    "                                              \"Preprocessing2\": d2, 'p': p})\n",
    "compare_between_preprocessing = pd.DataFrame(compare_between_preprocessing)\n",
    "\n",
    "compare_between_correction = []\n",
    "for dataset in preprocessings:\n",
    "    for i, postcorrection1 in enumerate(postcorrections[:-1]):\n",
    "        for postcorrection2 in postcorrections[i+1:]:\n",
    "            t, p = ttest_rel(recall_at_10fdr[postcorrection1][dataset], recall_at_10fdr[postcorrection2][dataset])\n",
    "            compare_between_correction.append({\"Correction1\": postcorrection1, \n",
    "                                               \"Correction2\": postcorrection2,\n",
    "                                              \"Preprocessing2\": dataset, \n",
    "                                               'p': p})\n",
    "compare_between_correction = pd.DataFrame(compare_between_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_between_preprocessing.groupby([\"Preprocessing1\", \"Preprocessing2\"]).p.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_between_correction.groupby([\"Correction1\", \"Correction2\"]).p.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmds = pd.concat([pd.DataFrame({'postcorrection': postcorrection, 'preprocessing': dataset, \n",
    "                                 'nnmd': val.apply(nnmd, axis=1).values,\n",
    "                                'line': val.index})\n",
    "                  for postcorrection, gene_effect in gene_effects.items()\n",
    "                for dataset, val in gene_effect.items()], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.violinplot(data=nnmds, x='preprocessing', hue='postcorrection', y='nnmd',\n",
    "               order=preprocessings, hue_order=postcorrections\n",
    "              )\n",
    "sns.despine(top=True, right=True)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"NNMD (lower is better)\")\n",
    "#plt.xticks(rotation=45)\n",
    "plt.gcf().set_size_inches(7.5, 5)\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + \"nnmd_violin.pdf\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmds.to_csv(output + 'nnmd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oncogene Biomarkers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Oncogenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    oncogenes = pd.read_csv('/Users/dempster/Documents/genes/oncokb_allvariants_20200715.csv',\n",
    "                       )\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\n",
    "\"You will need to download the OncoKB all variants annotation from \\\n",
    " http://oncokb.org/api/v1/utils/allAnnotatedVariants\\\n",
    " and save locally as a csv\"\n",
    "    )\n",
    "\n",
    "oncogenes = oncogenes[oncogenes.oncogenicity.isin(['Likely Oncogenic', 'Oncogenic'])]\n",
    "oncogenes = oncogenes[oncogenes['mutationEffect'].isin([\n",
    "    'Likely Gain-of-function','Gain-of-function',\n",
    "    'Likely Switch-of-function', 'Switch-of-function'\n",
    "])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_ess = gene_effects['ComBat']['CRISPRCleanR'].reindex(\n",
    "    columns=oncogenes.gene.unique()\n",
    ").median().loc[lambda x: x < -.5].index\n",
    "\n",
    "oncogenes = oncogenes[~oncogenes.gene.isin(already_ess)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_change_parse(s):\n",
    "    if pd.isnull(s):\n",
    "        return s\n",
    "    if s.startswith('p.'):\n",
    "        s = s[2:]\n",
    "    return s\n",
    "\n",
    "mut['Protein_Change'] = mut.Protein_Change.apply(p_change_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicated_alterations = {}\n",
    "for gene in oncogenes.gene.unique():\n",
    "    lines = set([])\n",
    "    sub = mut[mut.Hugo_Symbol == gene]\n",
    "    for alt in oncogenes.variant[oncogenes.gene == gene].unique():\n",
    "        lines = lines | set(sub[sub.Protein_Change == alt].DepMap_ID)\n",
    "        if alt.endswith('Fusions'):\n",
    "            lines = lines | set(fusions[fusions.LeftGene == gene].index)\\\n",
    "                        | set(fusions[fusions.RightGene == gene].index)\n",
    "        elif alt.endswith('Fusion'):\n",
    "            alt = alt.split('Fusion')[0]\n",
    "            pairs = tuple(sorted([s.strip() for s in alt.split('-')]))\n",
    "            lines = lines | set(fusions[fusions.Sorted == pairs].index)\n",
    "    indicated_alterations[gene] = lines\n",
    "\n",
    "dropped_alterations = [key for key, val in indicated_alterations.items()\n",
    "                        if len(val) == 0]\n",
    "indicated_alterations = {key: sorted(val) for key, val in indicated_alterations.items()\n",
    "                         if len(val) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alteration_matrix = pd.DataFrame(index=set.union(*[\n",
    "    set(val) for val in indicated_alterations.values()\n",
    "]))\n",
    "for gene, lines in indicated_alterations.items():\n",
    "    alteration_matrix[gene] = False\n",
    "    alteration_matrix.loc[lines, gene] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cannonical = []\n",
    "for postcorrection in postcorrections:\n",
    "    for dataset in preprocessings:\n",
    "        df = gene_effects[postcorrection][dataset]\n",
    "        gene_map = pd.Series(df.columns, index=[s.split(' ')[0] for s in df.columns])\n",
    "        for gene, lines in indicated_alterations.items():\n",
    "            if not gene in gene_map.index:\n",
    "                continue\n",
    "            lines_pos = sorted(set(lines) & set(df.index))\n",
    "            lines_neg = sorted(set(df.index) - set(lines))\n",
    "            cannonical.append(pd.DataFrame({\n",
    "                'postcorrection': postcorrection,\n",
    "                'dataset': dataset,\n",
    "                'gene': gene_map[gene],\n",
    "                'line': lines_pos,\n",
    "                'biomarker': True,\n",
    "                'gene_effect': df.loc[lines_pos, gene_map[gene]]\n",
    "            }))\n",
    "            cannonical.append(pd.DataFrame({\n",
    "                'postcorrection': postcorrection,\n",
    "                'dataset': dataset,\n",
    "                'gene': gene_map[gene],\n",
    "                'line': lines_neg,\n",
    "                'biomarker': False,\n",
    "                'gene_effect': df.loc[lines_neg, gene_map[gene]]\n",
    "            }))\n",
    "cannonical = pd.concat(cannonical, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cannonical.dropna(inplace=True)\n",
    "\n",
    "len(cannonical.gene.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oncogene_roc_auc = pd.DataFrame(columns=['Preprocessing', 'postcorrection', 'ROC AUC'])\n",
    "for dataset in preprocessings:\n",
    "    print(dataset)\n",
    "    for postcorrection in postcorrections:\n",
    "        sub = cannonical.query('postcorrection == %r' % postcorrection).query(\"dataset == %r\" % dataset)\n",
    "        yf = sub[sub.biomarker == False].gene_effect\n",
    "        yt = sub[sub.biomarker == True].gene_effect\n",
    "        auroc = roc_auc(yt, yf)#.sort_values()\n",
    "        oncogene_roc_auc = oncogene_roc_auc.append({\n",
    "            'Preprocessing': dataset, \n",
    "            'postcorrection': postcorrection,\n",
    "            'ROC AUC': auroc\n",
    "        }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oncogene_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=pd.concat([oncogene_roc_auc]*2, ignore_index=True),\n",
    "            x='Preprocessing', hue='postcorrection', y='ROC AUC',\n",
    "            order=preprocessings, hue_order=postcorrections)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "plt.ylim(.5, .82)\n",
    "plt.gca().get_legend().remove()\n",
    "plt.legend(loc=\"lower left\")\n",
    "sns.despine(right=True, top=True)\n",
    "plt.gcf().set_size_inches((4,4))\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + \"oncogene_roc_auc.pdf\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oncogene_roc_auc.groupby(\"Preprocessing\")['ROC AUC'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Gene Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnmd_reduce(group):\n",
    "    return (group[group.biomarker == True].gene_effect.mean() \n",
    "            - group[group.biomarker == False].gene_effect.mean()) \\\n",
    "          / group[group.biomarker == False].gene_effect.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_counts = cannonical.groupby([\"postcorrection\", \"dataset\", \"gene\"]).biomarker.sum()\\\n",
    "                    .loc[lambda x: x>1].index.get_level_values(2).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oncogene_nnmds = []\n",
    "for dataset in preprocessings:\n",
    "    print(dataset)\n",
    "    for postcorrection in postcorrections:\n",
    "        sub = cannonical.query('postcorrection == %r' % postcorrection).query(\"dataset == %r\" % dataset)\n",
    "        oncogene_nnmd = sub.groupby('gene').apply(nnmd_reduce).dropna().reset_index()\n",
    "        oncogene_nnmd[\"preprocessing\"] = dataset\n",
    "        oncogene_nnmd['postcorrection'] = postcorrection\n",
    "        oncogene_nnmds.append(oncogene_nnmd)\n",
    "oncogene_nnmds = pd.concat(oncogene_nnmds, ignore_index=True)\n",
    "oncogene_nnmds = oncogene_nnmds[oncogene_nnmds.gene.isin(positive_counts)]\n",
    "\n",
    "sns.boxplot(data=oncogene_nnmds, hue=\"postcorrection\", x=\"preprocessing\", y=0,\n",
    "           postcorrections=postcorrections, hue_order=preprocessings)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"NNMD\")\n",
    "plt.gca().minorticks_on()\n",
    "plt.gca().yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "plt.grid(axis='y', which=\"minor\")\n",
    "plt.grid(axis='y', which=\"major\")\n",
    "sns.despine(left=True, top=True)\n",
    "plt.gcf().set_size_inches((4, 4))\n",
    "plt.gca().get_legend().remove()\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + \"oncogene_per_gene_NNMD.pdf\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineage Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples=100\n",
    "disease_ami = {postcorrection: {} for postcorrection in gene_effects.keys()}\n",
    "var_genes = gene_effects['ComBat']['CERES'][shared_genes].std().sort_values()[-500:].index\n",
    "\n",
    "for dataset in preprocessings:\n",
    "    print(dataset)\n",
    "    \n",
    "    disease = pd.Series(pd.Categorical(si.loc[gene_effects['ComBat']['CERES'].index, \n",
    "                                              'lineage']).codes, \n",
    "                        index=gene_effects['ComBat']['CERES'].index)\n",
    "    disease = disease[~disease.index.duplicated()]\n",
    "    for key, val in gene_effects.items():\n",
    "        if not dataset in val.keys():\n",
    "            continue\n",
    "        disease_ami[key][dataset] = []\n",
    "        overlap = sorted(set(disease.index) & set(val[dataset].index))\n",
    "        v = val[dataset].loc[overlap, var_genes].copy()\n",
    "        v -= v.mean()\n",
    "        v /= v.std()\n",
    "        for gene in var_genes:\n",
    "            v[gene].loc[v.index[v[gene].isnull()]] = v[gene].mean()\n",
    "        model = KMeans(disease.max()+1)\n",
    "        for i in range(nsamples):\n",
    "            clusters = model.fit_predict(v)\n",
    "            disease_ami[key][dataset].append(\n",
    "                adjusted_mutual_info_score(disease.loc[v.index].astype(int).values,\n",
    "                                                               clusters.astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_ami = pd.concat([pd.DataFrame(\n",
    "    {'postcorrection': postcorrection, 'preprocessing': key, 'AMI': ami}\n",
    ") for postcorrection, da in disease_ami.items() \n",
    "for key, ami in da.items()\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_ami.to_csv(output + 'lineage_ami.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disease_ami.groupby(['preprocessing', 'postcorrection']).median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disease_ami.groupby(['postcorrection', 'preprocessing']).AMI.agg(lambda x: ttest_1samp(x, 0)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for processing1 in disease_ami.preprocessing.unique():\n",
    "    for processing2 in disease_ami.preprocessing.unique():\n",
    "        if not processing2 > processing1:\n",
    "            continue\n",
    "        pvalue = ttest_ind(disease_ami.query('preprocessing = =%r' % processing1).AMI,\n",
    "                           disease_ami.query('preprocessing = =%r' % processing2).AMI\n",
    "                          )[1]\n",
    "        print(processing1, processing2, pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.boxplot(data=disease_ami, x='preprocessing', y='AMI', hue='postcorrection',\n",
    "           order=preprocessings, hue_order=postcorrections)\n",
    "plt.ylabel(\"Adjusted Mutual Information\")\n",
    "sns.despine(top=True, right=True)\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + \"lineage_clustering.pdf\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Gene Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Munge related pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_genes = set.intersection(*[\n",
    "    set(ge.dropna(how='any', axis=1).columns)\n",
    "    for val in gene_effects.values()\n",
    "    for ge in val.values()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_pairs = set(related_genes.itertuples(index=False, name=None))\n",
    "\n",
    "gene_pairs = sorted([s for s in gene_pairs \n",
    "                  if s[0]>s[1] \n",
    "                  and s[0] in non_null_genes\n",
    "                  and s[1] in non_null_genes\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_pairs_reversed = sorted([(s[1], s[0]) for s in gene_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_means = {postcorrection: {key: val[non_null_genes].mean() for key, val in gene_effect.items()}\n",
    "              for postcorrection, gene_effect in gene_effects.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_mean_bins = {}\n",
    "gene_mean_binned = {}\n",
    "for postcorrection, means in gene_means.items():\n",
    "    gene_mean_bins[postcorrection] = {}\n",
    "    gene_mean_binned[postcorrection] = {}\n",
    "    for dataset, val in means.items():\n",
    "        binned_vals, bins = pd.cut(val, bins=20, retbins=True)\n",
    "        gene_mean_bins[postcorrection][dataset] = bins\n",
    "        gene_mean_binned[postcorrection][dataset] = binned_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_gene_means = {postcorrection: {dataset: .5*(\n",
    "    gene_means[postcorrection][dataset][[s[0] for s in gene_pairs]].values\n",
    "    + gene_means[postcorrection][dataset][[s[1] for s in gene_pairs]].values\n",
    ") for dataset in preprocessings} for postcorrection in postcorrections}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin related genes by mean effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition related pairs by the mean of the partner\n",
    "partner_gene_means = {postcorrection: {dataset: pd.Series(\n",
    "    gene_means[postcorrection][dataset][[s[1] for s in gene_pairs]].values,\n",
    "    index=gene_pairs)\n",
    "                            for dataset in preprocessings}\n",
    "                     for postcorrection in postcorrections}\n",
    "cuts = {postcorrection: {\n",
    "    dataset: pd.cut(val, bins=10) \n",
    "    for dataset, val in pgm.items()}\n",
    "        for postcorrection, pgm in partner_gene_means.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_pairs = {}\n",
    "for postcorrection in postcorrections:\n",
    "    null_pairs[postcorrection] = {}\n",
    "    print(postcorrection)\n",
    "    for key in preprocessings:\n",
    "        null_pairs[postcorrection][key] = []\n",
    "        bins = cuts[postcorrection][key].unique()\n",
    "        for b in bins:\n",
    "            ind = cuts[postcorrection][key].loc[lambda x: x == b].index\n",
    "            ind1, ind2 = zip(*list(ind))\n",
    "            ind1, ind2 = list(ind1), list(ind2)\n",
    "            np.random.shuffle(ind2)\n",
    "            null_pairs[postcorrection][key].extend(list(zip(ind1, ind2)))\\\n",
    "            \n",
    "        null_pairs[postcorrection][key] = sorted(\n",
    "                set(null_pairs[postcorrection][key]) \n",
    "              - set(gene_pairs) \n",
    "              - set(gene_pairs_reversed) \n",
    "              - set(zip( list(zip(*null_pairs[postcorrection][key]))[0],\n",
    "                                  list(zip(*null_pairs[postcorrection][key]))[0] \n",
    "                       ))\n",
    "            )\n",
    "        \n",
    "        print('\\t', key, len(null_pairs[postcorrection][key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find correlations for related and null pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_corrs = {postcorrection: {dataset:\n",
    "                         corr_dist(val[sorted(non_null_genes)],\n",
    "                                   {'true': gene_pairs, 'null': null_pairs[postcorrection][dataset]} \n",
    "                                  )\n",
    "                         for dataset, val in gene_effect.items()}\n",
    "                   for postcorrection, gene_effect in gene_effects.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues_gene = pd.DataFrame(columns=['Gene1', 'Gene2', 'postcorrection', 'preprocessing', 'p'])\n",
    "for postcorrection in gene_effects.keys():\n",
    "    for key in preprocessings:\n",
    "        null = gene_corrs[postcorrection][key]['null'].sort_values().dropna()\n",
    "        true = gene_corrs[postcorrection][key]['true'].sort_values().dropna()\n",
    "        pvals = 1 - (np.searchsorted(null, true)+1) / (len(null)+1)\n",
    "\n",
    "        pvalues_gene = pd.concat([pvalues_gene, pd.DataFrame({\n",
    "            'Gene1': [tup[0] for tup in true.index],\n",
    "            'Gene2': [tup[1] for tup in true.index],\n",
    "            'postcorrection': postcorrection,\n",
    "            'preprocessing': key,\n",
    "            'p': pvals,\n",
    "            'FDR': fdrcorrection(pvals, alpha=.05)[1]\n",
    "        })], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues_gene['FDR < .1'] = pvalues_gene.FDR < .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "axs = [axes[0, 0], axes[0, 1], axes[1, 0]]\n",
    "for dataset, ax in zip(preprocessings, axs):\n",
    "    plt.sca(ax)\n",
    "    print(dataset)\n",
    "    for postcorrection in postcorrections:\n",
    "        x = pvalues_gene\\\n",
    "            .query(\"preprocessing == %r\" % dataset)\\\n",
    "            .query(\"postcorrection == %r\" % postcorrection)\\\n",
    "            .FDR.dropna().sort_values()\n",
    "        y = np.linspace(0, 1, len(x))[x < .5]\n",
    "        x = x[x<.5]\n",
    "        plt.plot(x, y, label=postcorrection)\n",
    "        plt.xlabel(\"FDR\")\n",
    "        plt.ylabel(\"Recall of Relationships\")\n",
    "    if ax == axs[0]:\n",
    "        plt.legend()\n",
    "    sns.despine(right=True, top=True)\n",
    "    plt.title(dataset)\n",
    "        \n",
    "plt.sca(axes[1, 1])\n",
    "sns.barplot(data=pvalues_gene, x=\"preprocessing\", hue=\"postcorrection\", y=\"FDR < .1\",\n",
    "           order=preprocessings, hue_order=postcorrections, n_boot=100)\n",
    "sns.despine(top=True, right=True)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Recall 10% FDR\")\n",
    "plt.gca().get_legend().remove()\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output + \"gene_relationships.pdf\", dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
